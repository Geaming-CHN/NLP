{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度学习的文本分类\n",
    "\n",
    "使用基于mindspore框架的自然语言处理库mindnlp\n",
    "\n",
    "主仓库地址：https://github.com/mindspore-lab/mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造数据集\n",
    "\n",
    "根据所提供的news20数据集，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "news20 load function\n",
    "\"\"\"\n",
    "import os\n",
    "import mindspore\n",
    "from tqdm import tqdm\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "# mindspore.set_context(device_target=\"GPU\") # set GPU\n",
    "\n",
    "label_dict = {}\n",
    "label_count = 0\n",
    "\n",
    "class News20:\n",
    "    \"\"\"\n",
    "    NEWS dataset source\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path)->None:\n",
    "        self.path: str = path\n",
    "        self._text, self._label = [], []\n",
    "        self.label_dict = {}\n",
    "        self.label_count = 0\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        labels = os.listdir(self.path)\n",
    "        for label in labels:\n",
    "            self.label_dict[label] = self.label_count\n",
    "            self.label_count += 1\n",
    "            label_path = os.path.join(self.path, label)\n",
    "            with tqdm(total = len(os.listdir(label_path)), desc = label) as pbar:\n",
    "                for t in os.listdir(label_path):\n",
    "                    pbar.update(1)\n",
    "                    text_path = os.path.join(label_path, t)\n",
    "                    file = open(text_path, \"r\" ,encoding=\"latin-1\") # latin-1\n",
    "                    text = file.read()\n",
    "                    begin = text.find('\\n\\n')\n",
    "                    if 0 < begin: # skip head\n",
    "                        text = text[begin:]\n",
    "                    self._text.append(text)\n",
    "                    self._label.append(self.label_dict[label])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._text[index], self._label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._label)\n",
    "\n",
    "def news20(source:News20):\n",
    "    r\"\"\"\n",
    "    Load the news20 dataset\n",
    "    \"\"\"\n",
    "    column_names = [\"text\", \"label\"]\n",
    "    return GeneratorDataset(source = source, column_names = column_names, shuffle = False) # 不shuffle，划分数据集时会进行打乱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alt.atheism: 100%|██████████| 1000/1000 [00:00<00:00, 11594.77it/s]\n",
      "comp.graphics: 100%|██████████| 1000/1000 [00:00<00:00, 12359.82it/s]\n",
      "comp.os.ms-windows.misc: 100%|██████████| 1000/1000 [00:00<00:00, 11948.53it/s]\n",
      "comp.sys.ibm.pc.hardware: 100%|██████████| 1000/1000 [00:00<00:00, 11998.81it/s]\n",
      "comp.sys.mac.hardware: 100%|██████████| 1000/1000 [00:00<00:00, 11137.23it/s]\n",
      "comp.windows.x: 100%|██████████| 1000/1000 [00:00<00:00, 12160.50it/s]\n",
      "misc.forsale: 100%|██████████| 1000/1000 [00:00<00:00, 11795.11it/s]\n",
      "rec.autos: 100%|██████████| 1000/1000 [00:00<00:00, 11796.23it/s]\n",
      "rec.motorcycles: 100%|██████████| 1000/1000 [00:00<00:00, 11933.06it/s]\n",
      "rec.sport.baseball: 100%|██████████| 1000/1000 [00:00<00:00, 11560.51it/s]\n",
      "rec.sport.hockey: 100%|██████████| 1000/1000 [00:00<00:00, 11018.42it/s]\n",
      "sci.crypt: 100%|██████████| 1000/1000 [00:00<00:00, 11379.97it/s]\n",
      "sci.electronics: 100%|██████████| 1000/1000 [00:00<00:00, 11936.63it/s]\n",
      "sci.med: 100%|██████████| 1000/1000 [00:00<00:00, 9459.14it/s]\n",
      "sci.space: 100%|██████████| 1000/1000 [00:00<00:00, 9198.92it/s]\n",
      "soc.religion.christian: 100%|██████████| 997/997 [00:00<00:00, 11107.42it/s]\n",
      "talk.politics.guns: 100%|██████████| 1000/1000 [00:00<00:00, 8718.93it/s]\n",
      "talk.politics.mideast: 100%|██████████| 1000/1000 [00:00<00:00, 9734.68it/s]\n",
      "talk.politics.misc: 100%|██████████| 1000/1000 [00:00<00:00, 9033.24it/s]\n",
      "talk.religion.misc: 100%|██████████| 1000/1000 [00:00<00:00, 9031.82it/s]\n",
      "[WARNING] ME(6440:11900,MainProcess):2022-12-17-14:24:42.299.275 [mindspore\\dataset\\engine\\datasets_user_defined.py:657] Python multiprocessing is not supported on Windows platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}\n"
     ]
    }
   ],
   "source": [
    "# 载入数据\n",
    "dataset_source = News20(r\"G:\\_CQU\\3-1\\自然语言处理\\实验\\实验二\\NLP\\news20\\20_newsgroup\")\n",
    "print(dataset_source.label_dict)\n",
    "dataset = news20(dataset_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Models & Loss & Optimizer\n",
    "hidden_size = 256\n",
    "output_size = 20\n",
    "num_layers = 2\n",
    "bidirectional = False\n",
    "drop = 0.5\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "import mindspore\n",
    "from mindspore.dataset import text, transforms\n",
    "from mindnlp.modules import Glove\n",
    "from mindnlp.dataset.transforms import BasicTokenizer\n",
    "from mindnlp.dataset import process\n",
    "\n",
    "def data_process(dataset, tokenizer, vocab, batch_size = 64, max_len = 1000, drop_remainder = False):\n",
    "    \"\"\"\n",
    "    预处理\n",
    "    \"\"\"\n",
    "    # print(\"!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    dataset = dataset.map([tokenizer], 'text')\n",
    "    lookup_op = text.Lookup(vocab, unknown_token = '<unk>')\n",
    "    dataset = dataset.map([lookup_op], 'text')\n",
    "    pad_value = vocab.tokens_to_ids('<pad>')\n",
    "    pad_op = transforms.PadEnd([max_len], pad_value)\n",
    "    dataset = dataset.map([pad_op], 'text')\n",
    "    onehot_op = transforms.OneHot(num_classes=20)\n",
    "    dataset = dataset.map([onehot_op], 'label')\n",
    "    type_cast_op = transforms.TypeCast(mindspore.float32)\n",
    "    dataset = dataset.map([type_cast_op], 'label')\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder = drop_remainder)\n",
    "    return dataset\n",
    "\n",
    "tokenizer = BasicTokenizer(True)\n",
    "embedding, vocab = Glove.from_pretrained('6B', 100, special_tokens=[\"<unk>\", \"<pad>\"], dropout=drop)\n",
    "\n",
    "dataset_process = data_process(dataset, tokenizer, vocab) # 其余使用默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_dataset, test_dataset = dataset_process.split([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "import math\n",
    "\n",
    "from mindspore import nn\n",
    "from mindspore import ops\n",
    "from mindspore.common.initializer import Uniform, HeUniform\n",
    "from mindnlp.abc import Seq2vecModel\n",
    "\n",
    "class Head(nn.Cell):\n",
    "    \"\"\"\n",
    "    Head for Classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        weight_init = HeUniform(math.sqrt(5))\n",
    "        bias_init = Uniform(10 / math.sqrt(hidden_dim * 2))\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim, weight_init=weight_init, bias_init=bias_init)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(1 - dropout)\n",
    "\n",
    "    def construct(self, context):\n",
    "        context = ops.concat((context[-2, :, :], context[-1, :, :]), axis=1)\n",
    "        context = self.dropout(context)\n",
    "        return self.softmax(self.fc(context))\n",
    "\n",
    "\n",
    "class Classification(Seq2vecModel):\n",
    "    \"\"\"\n",
    "    Classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, head):\n",
    "        super().__init__(encoder, head)\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def construct(self, text):\n",
    "        _, (hidden, _), _ = self.encoder(text)\n",
    "        output = self.head(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/250 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error. map operation: [Lookup] failed. Lookup: input is not string datatype.\nLine of code : 28\nFile         : mindspore\\ccsrc\\minddata\\dataset\\text\\kernels\\lookup_op.cc\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6440\\2862876824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer = Trainer(network=net, train_dataset = train_dataset, eval_dataset = test_dataset, metrics=metric,\n\u001b[0;32m     19\u001b[0m                   epochs=5, loss_fn=loss, optimizer=optimizer)\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"end train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\_CQU\\3-1\\自然语言处理\\实验\\实验二\\NLP\\mindnlp\\engine\\trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, tgt_columns, jit)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mrun_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\_CQU\\3-1\\自然语言处理\\实验\\实验二\\NLP\\mindnlp\\engine\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, run_context, tgt_columns, jit)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;31m# step begin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dict_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                     \u001b[0mrun_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_step_nums\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda\\envs\\1216\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda\\envs\\1216\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py\u001b[0m in \u001b[0;36m_get_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Memory error occurred, process will exit.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda\\envs\\1216\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py\u001b[0m in \u001b[0;36m_get_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffload_model\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_md_to_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetNextAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_md_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetNextAsList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unexpected error. map operation: [Lookup] failed. Lookup: input is not string datatype.\nLine of code : 28\nFile         : mindspore\\ccsrc\\minddata\\dataset\\text\\kernels\\lookup_op.cc\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.modules import RNNEncoder\n",
    "from mindnlp.engine.metrics import Accuracy\n",
    "from mindnlp.engine.trainer import Trainer\n",
    "\n",
    "lstm_layer = nn.LSTM(100, hidden_size, num_layers=num_layers, batch_first=True,\n",
    "                     dropout=drop, bidirectional=bidirectional)\n",
    "encoder = RNNEncoder(embedding, lstm_layer)\n",
    "head = Head(hidden_size, output_size, drop)\n",
    "\n",
    "net = Classification(encoder, head)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)\n",
    "\n",
    "# define metrics\n",
    "metric = Accuracy()\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(network=net, train_dataset = train_dataset, eval_dataset = test_dataset, metrics=metric,\n",
    "                  epochs=5, loss_fn=loss, optimizer=optimizer)\n",
    "trainer.run(tgt_columns=\"label\", jit=False)\n",
    "print(\"end train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('1216')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77f8c42b9048a804c9b54fe3fdcc803144c07769274c7f5b50f37195cc1e314b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
